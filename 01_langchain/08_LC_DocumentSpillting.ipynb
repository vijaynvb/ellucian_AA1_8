{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "J4Mv09eTNpDz",
      "metadata": {
        "id": "J4Mv09eTNpDz"
      },
      "source": [
        "# **Document Splitting**\n",
        "\n",
        "Document splitting is crucial because it ensures that semantically relevant content is grouped together within the same chunk. This is particularly important when answering questions or performing other tasks that rely on the contextual information present in the documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5tL8w9jlQx9Y",
      "metadata": {
        "id": "5tL8w9jlQx9Y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "%pip install --upgrade langchain_classic langchain_community pypdf tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d004c36b",
      "metadata": {
        "id": "d004c36b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = os.getenv(\"AWS_DEFAULT_REGION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d3b0e2f6",
      "metadata": {
        "id": "d3b0e2f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ysx_tvIuOZIY",
      "metadata": {
        "id": "Ysx_tvIuOZIY"
      },
      "source": [
        "## **RecursiveCharacterTextSplitter**\n",
        "\n",
        "The RecursiveCharacterTextSplitter is recommended for generic text splitting. It splits the text based on a hierarchy of separators, starting with double newlines (\\n\\n), then single newlines (\\n), spaces ( ), and finally, individual characters. This approach aims to preserve the structure and coherence of the text by prioritizing splitting at natural boundaries like paragraphs and sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b80190a8",
      "metadata": {
        "id": "b80190a8"
      },
      "outputs": [],
      "source": [
        "chunk_size = 15\n",
        "chunk_overlap = 4\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "141da019",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "141da019",
        "outputId": "175586ec-352d-40ac-cda5-0d10a7e59e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['employeemanagem', 'agementsystem']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1 = 'employeemanagementsystem'\n",
        "\n",
        "r_splitter.split_text(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Bzi_gCs6SiUd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzi_gCs6SiUd",
        "outputId": "957e2861-4a98-4120-f186-72afe9818b0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['employeemanagem', 'agementsystemar', 'emarch']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text2 = 'employeemanagementsystemarch'\n",
        "\n",
        "r_splitter.split_text(text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PLIBa5-3Oseg",
      "metadata": {
        "id": "PLIBa5-3Oseg"
      },
      "source": [
        "## **CharacterTextSplitter**\n",
        "\n",
        "The CharacterTextSplitter is a more basic splitter that splits the text based on a single character separator, such as a space or a newline. This splitter is useful when dealing with text that doesn't have a clear structure or when you want to split the text at specific points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "H_Q3D4vnOkiZ",
      "metadata": {
        "id": "H_Q3D4vnOkiZ"
      },
      "outputs": [],
      "source": [
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "028320a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "028320a0",
        "outputId": "205a2bfe-b979-4ca5-83c4-133330a29fd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['e m p l o y e e', 'e e m a n a g e', 'g e m e n t s y', 's y s t e m']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text3 = \"e m p l o y e e m a n a g e m e n t s y s t e m\"\n",
        "r_splitter.split_text(text3)\n",
        "c_splitter.split_text(text3)\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separator = ' '\n",
        ")\n",
        "c_splitter.split_text(text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LUhtRh_RTiA1",
      "metadata": {
        "id": "LUhtRh_RTiA1"
      },
      "source": [
        "# **Recursive splitting details**\n",
        "\n",
        "\n",
        "RecursiveCharacterTextSplitter is recommended for generic text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b1bc9199",
      "metadata": {
        "id": "b1bc9199"
      },
      "outputs": [],
      "source": [
        "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
        "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
        "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
        "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
        "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
        "Sentences have a period at the end, but also, have a space.\\\n",
        "and words are separated by space.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7235e106",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7235e106",
        "outputId": "4b16c6f5-9130-4e84-f82a-27bbe7dd2f3b"
      },
      "outputs": [],
      "source": [
        "len(some_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27bc4657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27bc4657",
        "outputId": "7833ba17-c76f-46ae-fc97-77af45c0b6bc"
      },
      "outputs": [],
      "source": [
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size=450,\n",
        "    chunk_overlap=0,\n",
        "    separator = ' '\n",
        ")\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=343,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "c_splitter.split_text(some_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1129f129",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1129f129",
        "outputId": "18c498e2-cd5f-4e06-de23-9c6680c69fb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
              " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
              " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
              " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "r_splitter.split_text(some_text)\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,\n",
        "    chunk_overlap=0,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=. )\", \" \", \"\"],\n",
        "    is_separator_regex=True\n",
        ")\n",
        "r_splitter.split_text(some_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e128d8c0",
      "metadata": {
        "id": "e128d8c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ignoring wrong pointing object 6 0 (offset 0)\n",
            "Ignoring wrong pointing object 9 0 (offset 0)\n",
            "Ignoring wrong pointing object 12 0 (offset 0)\n",
            "Ignoring wrong pointing object 23 0 (offset 0)\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"./content/Vijay Trainer Profile_Sep_25.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "308e62c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "308e62c3",
        "outputId": "6f2c21f8-aa9e-45f7-ab38-71da15b6bb53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_classic.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "len(docs)\n",
        "#len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3052d9dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='Trainer Details   Vijaya Bhaskar Reddy N         Email: vijay@swayaan.com Phone Number: +919845733399   Summary  Total Experience: 19 Years || Training Delivery: 10+ Years || Total Trainings: 500+ Highly motivated technology trainer with 19+ years of experience in training small and large groups across diverse industries primarily on DevOps, Azure, AWS, GCP, APIs, Micro services, SOA, Python, Testing, Snowflake, Performance Testing, Jmeter, Cucumber & Cloud Computing. Proven success in leveraging educational theories and methodologies to design, develop, and deliver successful training programs and integrate instructional technology to provide onsite and virtual training. Efficient trainings delivered with real life use cases and scenarios  Expertise   • GAI, Prompt Engineering, Deep Learning, ML, AI, LLM’s, SLM, MLOps, LLMOps, Responsible AI.  • Agentic AI: Langchain, LangGraph, Lang Smith, [Eval], Arize.AI, RAGA’s,  AWS Bedrock, Agents, AWS Sagemaker, Azure AI Foundry, Semantic Kernel SDK, AzureML. • Code Assistant AI: Github Copilot [Certified], Tabnine, Amazon Q AWS Whisper, coursor_ai, windsurf_ai. • AI Tools: Microsoft Office Copilot, OpenAI ChatGPT, Google Gemini, Gamma,  • NLP, TensorFlow, Image Processing, Speech Processing, Nuance SDK. • Azure Cloud Platform, Amazon Web Services, Google Cloud Platform. • Dot.Net Core, Entity framework Core, Micro Services Development • Micro Services with Kubernetes, Docker, Helm, Istio • DevOps tools - Terraform, Codefresh, Crossplane, Ansible, GitHub Actions, Openshift, Splunk • CI/CD Integration Testing: SonarQube, SonarCloud, GHA Matrix for Infrastructure and Clean Cloud Best Practices Validations. • Web Services and SOA [SOAP and REST] • Workflow Orchestration in Microservices Architecture • C, C++ Development, Deployment and Debugging • Core Java, J2EE, Spring & Spring Boot  • C#, .Net Framework, .Net Core, ASP.Net, MAUI, Blazer, Signal R, WPF'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='• HTML5, Java Script and CSS3 • Node JS and Angular, RxJS, React JS, Redux, Meteor Framework • Cordova Mobile Development, React Native, Xamarin. • MySQL, Oracle, MSSQL • MongoDB, CouchDB, Neo4j • Rabbit MQ, Kafka, MSMQ • API Management Gateways, KONG, APIGEE Edge [Certified], Hashicorp • Agile Methodology for Service Delivery • Best innovation award at APIGEE using APIGEE EDGE gateway. • GCP Services, Cloud SQL, BIG Query, Tensor Flow, GKE etc • TDD, BDD, Hybrid test case writing, Test artifacts, Traceability Matrix, Regression Testing, Defect Management. • Postman, Robot Framework, Automation Frameworks (Data Driven, Keyword, Hybrid), Page Object Model (POM), SOAP UI. • Behavior Driven Development: Cucumber, Gherkin, Behave – Feature files, Step Definitions, Hooks, Tags, Test Reporting. • Performance Testing: JMeter, LoadRunner – Service Discovery, DB Performance Matrices, Test Automation Reports. • Cloud-Based Testing: Chaos Engineering with Chaos Monkey, Architecture, Data Pipelines with Snowpipe & Tasks, Streaming, Lambda-based automation, Performance Tuning, Cost Optimization.'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Deliveries  20+ Trainings on GAI, Code Assistants, Agentic AI’s. 300+ training programs delivered on Docker Kubernetes Helm Istio(30+), Microservices (20+), Azure Services (20+), Azure DevOps (10+), Azure Kubernetes Service (10+), AWS Elastic Kubernetes Service(5+), OpenShift (5+), AWS (20+), GCP (10+), .Net Core & Microservices (10+), Java/J2EE (10+), REST & APIGEE(10+), React (20+), Angular (10+), etc   SL No  Technology  Client Overall No Of trainings  GAI Hexaware, Ellucian, Accenture, Carelon, Aristocart, CapGemeni 20+ \\n Github Copilot Hexaware, Ellucian, CapGemeni, Aristocart. 20+ \\n Agentic AI + LangGraph, LangSmit Ellucian, Carelon, Hexaware, CapGemeni 10+ \\n AWS Agentic AI Ellucian, Aristocart 5+ \\n Azure Agentic AI Accenture, Webinars 2 \\n Reasonable AI Hexaware, Ellucian, Aristocart 5 \\n Prompt Engg Hexaware, Ellucian, Accenture, CapGemeni 5'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Azure Agentic AI Accenture, Webinars 2 \\n Reasonable AI Hexaware, Ellucian, Aristocart 5 \\n Prompt Engg Hexaware, Ellucian, Accenture, CapGemeni 5 \\n 1 Java Full stack Programming and Testing (SDET) Cyient, Collabra US, Mobily, IBM Philippines, IBM Singapore, Accenture, Opturm, Versa Network, IBM ISL, Unisys, EY, CBA  50+ 2 Java Commonwealth Bank, IBM, Accenture 50+  3  Spring Boot Cyient, Collabra US, Mobily, IBM Philippines, IBM Singapore, Opturm, Kyndryl, Travelex  50+  4  Spring Cloud Cyient, Collabra US, Mobily, IBM Philippines, IBM Singapore  20+  5  Angular, React, Meteor.js, Next.js, Vue.js  Trinity, Trianz, Optrum, Cyient, Siemens, IBM, Accenture, CBA  50+'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='6  HTML5, CSS3, Advanced JavaScript  Accenture, Kyndryl, Optum, Collabra US  100+  7  Dot.Net Core PJIL, Metrobank, Trianz, Synechron, Provana, Accenture Singapore, Commonwealth Bank  50+  8 C#, Asp.Net Core, Entity Framework, NUnit, Selenium  PJIL, Metrobank, Trianz, Synechron, Provana, Accenture Singapore, Commonwealth Bank  50+ 9 MAUI  2 11 WPF Developer Training  5 12 React native  5 13 Microservices Thinkwright , IBM, Dell, VM Ware 50+ 14 Design Patterns Brillio, Piramal, Provana 10+ 15 WinDBG Light & Wonder 1  16 C++, Advanced C++, Microservices  Wipro, Collabera US, Siemens, Scientific Games, Cyient  20+  17  Docker Kubernetes Helm Cyient, Ellucian, Ericsson, DELL, VMware, Unisys, Travelex, IBM, Synechron, Brillio, ANZ, ANZ  100+ 18 OpenShift Brillio 5  19  APIGEE Allianz, Airtel, Tory Haris, Schlumberger, BNP Paribas, AllState, World Bank,  25+ 20 Kong Siemens, Hexaware , JP Morgan and Chase 10'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='21 Jenkins Ellucian, DELL, Unisys, ANZ 10 22 Codefresh ANZ 2 23 GitHub Actions ANZ 10 24 Ansible ANZ, Unisys , Travelex 10 25 Terraform ANZ, Unisys , Travelex 10 26 Python Hexaware, Ellucain, Accenture, Optum, Trianz, IBM 10 27 NLP, AI, ML Hexaware 5  31 GitHub Co-pilot VS code, IntelliJ  Hexaware  5 32 AWS Wisper Hexaware 5 33 Microsoft Office Co-pilot Hexaware 5 34 MS SQL Accenture Singapore 50+ 35 MySQL Trianz 50+ 36 NoSQL Trianz 50+ 37 Couch DB CGI 5 38 MongoDB Siemens 20+ 39 Neo4j Hexaware, IBM 5 40 AWS Services Ellucian 20+ 41 Azure Services Accenture Singapore, IBM, Ericsson, Amdocs 20+ 42 Istio Service Mesh Cyient, DELL 10+ 43 Jira Service Management  5 44 Rabbit MQ Ellucian, Society General 20+ 45 Kafka Siemens 5+ 46 Unix and OS Virtusa, Versa Networks 10+ 47 AWS EKS Brillio, Travelex 10+ 48 KeyCloak Siemens, Brillio 5+ 49 Couch Base Amdocs 5 52 Splunk ANZ 5 53 Clean Code and TDD Piramal  10+    Experience  • Co-Founder of Swayaan Digital Solutions Pvt Ltd • Lead Technical Consultant @ Thomson Reuters. • Principle Developer @ Nuance Medical Transcription Pvt Ltd. • Solution Architect/Evangelist @ Focus Softek Pvt Ltd'),\n",
              " Document(metadata={'producer': 'macOS Version 14.3.1 (Build 23D60) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20250917112356Z00'00'\", 'title': 'Microsoft Word - Vijay Trainer Profile_May_25.docx', 'moddate': \"D:20250917112356Z00'00'\", 'source': './content/Vijay Trainer Profile_Sep_25.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='Education & Certifications  • BE in Computer Science - VTU, Bangalore. 2007')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VJf2gLvaTyos",
      "metadata": {
        "id": "VJf2gLvaTyos"
      },
      "source": [
        "# **Token splitting**\n",
        "\n",
        "The TokenTextSplitter splits the text based on token count rather than character count. This can be useful because many language models have context windows designated by token count rather than character count. Tokens are often approximately four characters long, so splitting based on token count can provide a better representation of how the language model will process the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c1cadf",
      "metadata": {
        "id": "c4c1cadf"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.text_splitter import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644b957d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "644b957d",
        "outputId": "c338fb4f-fa8d-4fd7-e311-8100da58d59b"
      },
      "outputs": [],
      "source": [
        "text1 = \"Hello World\"\n",
        "text_splitter.split_text(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8894645",
      "metadata": {
        "id": "f8894645"
      },
      "outputs": [],
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d9d54a",
      "metadata": {
        "id": "24d9d54a"
      },
      "outputs": [],
      "source": [
        "docs = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67ec33a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67ec33a",
        "outputId": "d000e843-c0d4-401f-a43a-80795279b938"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3987d89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3987d89",
        "outputId": "30355c3c-a619-430f-de04-42246c8bc214"
      },
      "outputs": [],
      "source": [
        "pages[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9XshdUJfPoc_",
      "metadata": {
        "id": "9XshdUJfPoc_"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Practice document splitting techniques with LangChain to manage large text content effectively. You will learn to use different splitters to break down text into manageable chunks, which is essential for tasks like text analysis, summarization, and feeding content into language models.\n",
        "\n",
        "## **Scenario**\n",
        "\n",
        "You are developing a text analysis module that processes large documents. This activity will help you understand how to use various text splitting techniques in LangChain to handle large text inputs efficiently.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* Load a Sample Document\n",
        "* RecursiveCharacterTextSplitter\n",
        "\n",
        "  * Use `RecursiveCharacterTextSplitter` to split the text based on a hierarchy of separators.\n",
        "  * Set a `chunk_size` and `chunk_overlap` to see how the text is divided.\n",
        "  * Experiment with different separators to observe how the splitting changes.\n",
        "\n",
        "* CharacterTextSplitter\n",
        "\n",
        "  * Use `CharacterTextSplitter` to split the text based on a single character separator.\n",
        "  * Compare the results with the recursive splitter to understand the differences.\n",
        "\n",
        "* TokenTextSplitter\n",
        "\n",
        "  * Use `TokenTextSplitter` to split the text based on token count.\n",
        "  * Set a `chunk_size` and `chunk_overlap` to see how the text is divided.\n",
        "  * Understand the importance of token-based splitting for language model processing.\n",
        "\n",
        "* Exploration and Analysis\n",
        "\n",
        "  * Experiment with different `chunk_size` and `chunk_overlap` settings to see how they affect the splitting.\n",
        "  * Use various text samples and documents to explore the effectiveness of each splitting technique."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
